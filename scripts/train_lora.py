"""
LoRA Fine-tuning Script for Yuanqi Pulse Method AI
å…ƒæ°”è„‰æ³•AIå¾®è°ƒè®­ç»ƒè„šæœ¬

Uses LoRA (Low-Rank Adaptation) for efficient fine-tuning on local hardware.
Supports both CPU and GPU training (GPU recommended).

Requirements:
    pip install transformers peft datasets accelerate bitsandbytes

Usage:
    python scripts/train_lora.py --model qwen --epochs 3
"""

import os
import sys
import json
import argparse
from datetime import datetime

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–"""
    missing = []
    try:
        import torch
    except ImportError:
        missing.append("torch")
    try:
        import transformers
    except ImportError:
        missing.append("transformers")
    try:
        import peft
    except ImportError:
        missing.append("peft")
    try:
        import datasets
    except ImportError:
        missing.append("datasets")
    
    if missing:
        print(f"âš ï¸  ç¼ºå°‘ä¾èµ–: {', '.join(missing)}")
        print(f"è¯·è¿è¡Œ: pip install {' '.join(missing)}")
        return False
    return True


def load_training_data(data_path: str):
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    from datasets import Dataset
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # è½¬æ¢ä¸ºDatasetæ ¼å¼
    dataset = Dataset.from_list(data)
    return dataset


def format_prompt(example):
    """æ ¼å¼åŒ–ä¸ºè®­ç»ƒprompt"""
    instruction = example.get("instruction", "")
    input_text = example.get("input", "")
    output = example.get("output", "")
    
    if input_text:
        text = f"""### æŒ‡ä»¤ï¼š
{instruction}

### è¾“å…¥ï¼š
{input_text}

### å›ç­”ï¼š
{output}"""
    else:
        text = f"""### æŒ‡ä»¤ï¼š
{instruction}

### å›ç­”ï¼š
{output}"""
    
    return {"text": text}


def train_lora(
    model_name: str = "Qwen/Qwen2.5-0.5B",
    data_path: str = "data/training/alpaca_yuanqi.json",
    output_dir: str = "models/yuanqi_lora",
    epochs: int = 3,
    batch_size: int = 2,
    learning_rate: float = 2e-4,
    use_4bit: bool = True
):
    """
    ä½¿ç”¨LoRAå¾®è°ƒæ¨¡å‹
    
    Args:
        model_name: åŸºç¡€æ¨¡å‹åç§°
        data_path: è®­ç»ƒæ•°æ®è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        learning_rate: å­¦ä¹ ç‡
        use_4bit: æ˜¯å¦ä½¿ç”¨4bité‡åŒ–
    """
    import torch
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        TrainingArguments,
        Trainer,
        DataCollatorForLanguageModeling
    )
    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
    
    print("=" * 50)
    print("å…ƒæ°”è„‰æ³•AI LoRAå¾®è°ƒè®­ç»ƒ")
    print("=" * 50)
    
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # æ¨¡å‹åŠ è½½é…ç½®
    model_kwargs = {
        "trust_remote_code": True,
        "device_map": "auto" if device != "cpu" else None,
    }
    
    if use_4bit and device == "cuda":
        from transformers import BitsAndBytesConfig
        model_kwargs["quantization_config"] = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
    
    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
    
    if use_4bit and device == "cuda":
        model = prepare_model_for_kbit_training(model)
    
    # LoRAé…ç½®
    print("\nğŸ”§ é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
    dataset = load_training_data(data_path)
    
    # æ ¼å¼åŒ–æ•°æ®
    dataset = dataset.map(format_prompt)
    
    # åˆ†è¯
    def tokenize(example):
        return tokenizer(
            example["text"],
            truncation=True,
            max_length=512,
            padding="max_length"
        )
    
    tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)
    
    print(f"   è®­ç»ƒæ ·æœ¬: {len(tokenized_dataset)}")
    
    # è®­ç»ƒå‚æ•°
    os.makedirs(output_dir, exist_ok=True)
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=4,
        learning_rate=learning_rate,
        warmup_steps=10,
        logging_steps=10,
        save_steps=50,
        save_total_limit=2,
        fp16=device == "cuda",
        report_to="none",
        remove_unused_columns=False
    )
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    # è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"   è½®æ•°: {epochs}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    info = {
        "base_model": model_name,
        "trained_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "epochs": epochs,
        "samples": len(tokenized_dataset),
        "device": device
    }
    with open(os.path.join(output_dir, "training_info.json"), 'w') as f:
        json.dump(info, f, ensure_ascii=False, indent=2)
    
    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    return output_dir


def test_model(model_path: str, prompt: str):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel
    
    print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model_path}")
    
    # åŠ è½½è®­ç»ƒä¿¡æ¯
    info_path = os.path.join(model_path, "training_info.json")
    with open(info_path, 'r') as f:
        info = json.load(f)
    
    base_model = info["base_model"]
    
    # åŠ è½½æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    base = AutoModelForCausalLM.from_pretrained(base_model, trust_remote_code=True)
    model = PeftModel.from_pretrained(base, model_path)
    
    # ç”Ÿæˆ
    formatted_prompt = f"""### æŒ‡ä»¤ï¼š
{prompt}

### å›ç­”ï¼š
"""
    
    inputs = tokenizer(formatted_prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7,
        do_sample=True
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\nğŸ“ å›ç­”:\n{response}")


def main():
    parser = argparse.ArgumentParser(description="å…ƒæ°”è„‰æ³•AI LoRAå¾®è°ƒè®­ç»ƒ")
    parser.add_argument("--model", default="Qwen/Qwen2.5-0.5B", help="åŸºç¡€æ¨¡å‹")
    parser.add_argument("--data", default="data/training/alpaca_yuanqi.json", help="è®­ç»ƒæ•°æ®")
    parser.add_argument("--output", default="models/yuanqi_lora", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=2, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•å·²è®­ç»ƒæ¨¡å‹")
    parser.add_argument("--prompt", default="è¯·è§£é‡Šå…ƒæ°”è„‰æ³•ä¸­çš„è„‰è±¡åˆ†ææ–¹æ³•", help="æµ‹è¯•prompt")
    
    args = parser.parse_args()
    
    if not check_dependencies():
        return
    
    if args.test:
        test_model(args.output, args.prompt)
    else:
        train_lora(
            model_name=args.model,
            data_path=args.data,
            output_dir=args.output,
            epochs=args.epochs,
            batch_size=args.batch_size
        )


if __name__ == "__main__":
    main()
