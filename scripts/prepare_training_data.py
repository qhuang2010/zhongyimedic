"""
Training Data Preparation for Yuanqi Pulse Method AI
å…ƒæ°”è„‰æ³•AIè®­ç»ƒæ•°æ®å‡†å¤‡

Converts imported knowledge base data into training format for:
1. Knowledge injection (RAG)
2. Fine-tuning (LoRA/QLoRA)
3. Prompt-based learning

Output Formats:
- JSONL for fine-tuning
- Embeddings for RAG
"""

import os
import sys
import json
import yaml
from typing import Dict, List, Any, Optional
from datetime import datetime

# Add src to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def load_imported_data(import_dir: str) -> Dict[str, List[Dict]]:
    """
    åŠ è½½æ‰€æœ‰å·²å¯¼å…¥çš„æ•°æ®
    Load all imported data
    """
    data = {
        "theories": [],
        "cases": []
    }
    
    for filename in os.listdir(import_dir):
        if not filename.endswith('.json'):
            continue
        
        filepath = os.path.join(import_dir, filename)
        with open(filepath, 'r', encoding='utf-8') as f:
            items = json.load(f)
        
        if 'theories' in filename:
            data["theories"].extend(items)
        elif 'general' in filename:  # Case data
            data["cases"].extend(items)
    
    return data


def generate_theory_prompts(theories: List[Dict]) -> List[Dict[str, str]]:
    """
    å°†ç†è®ºè½¬æ¢ä¸ºé—®ç­”å¯¹ï¼ˆç”¨äºå¾®è°ƒï¼‰
    Convert theories to Q&A pairs for fine-tuning
    """
    prompts = []
    
    for theory in theories:
        title = theory.get("title", "")
        content = theory.get("content", "")
        
        if not title or not content:
            continue
        
        # ç”Ÿæˆé—®ç­”å¯¹
        qa = {
            "instruction": f"è¯·è¯¦ç»†è§£é‡Šå…ƒæ°”è„‰æ³•ä¸­å…³äºã€{title}ã€‘çš„ç†è®ºã€‚",
            "input": "",
            "output": content.strip()
        }
        prompts.append(qa)
        
        # ç”Ÿæˆç®€ç­”ç‰ˆæœ¬
        if len(content) > 200:
            summary = content[:200] + "..."
            qa_short = {
                "instruction": f"ç®€è¿°å…ƒæ°”è„‰æ³•ã€{title}ã€‘çš„æ ¸å¿ƒè¦ç‚¹ã€‚",
                "input": "",
                "output": summary
            }
            prompts.append(qa_short)
    
    return prompts


def generate_case_prompts(cases: List[Dict]) -> List[Dict[str, str]]:
    """
    å°†ç—…ä¾‹è½¬æ¢ä¸ºè¯Šæ–­æ¨ç†å¯¹ï¼ˆç”¨äºå¾®è°ƒï¼‰
    Convert cases to diagnostic reasoning pairs
    """
    prompts = []
    
    for case in cases:
        title = case.get("title", "")
        content = case.get("content", "")
        
        if not content:
            continue
        
        # å°è¯•æå–ç»“æ„åŒ–ä¿¡æ¯
        # ç”Ÿæˆè¯Šæ–­é—®ç­”
        qa = {
            "instruction": "æ ¹æ®ä»¥ä¸‹ç—…ä¾‹ä¿¡æ¯ï¼Œè¯·æŒ‰ç…§å…ƒæ°”è„‰æ³•è¿›è¡Œè¾¨è¯åˆ†æå’Œå¤„æ–¹å»ºè®®ã€‚",
            "input": f"ç—…ä¾‹ï¼š{title}\n\nä¸´åºŠèµ„æ–™ï¼š{content[:500]}...",
            "output": f"## å…ƒæ°”è„‰æ³•è¾¨è¯åˆ†æ\n\n{content}"
        }
        prompts.append(qa)
    
    return prompts


def generate_cot_prompts(cases: List[Dict]) -> List[Dict[str, str]]:
    """
    ç”Ÿæˆæ€ç»´é“¾æ¨ç†æ•°æ®ï¼ˆChain-of-Thoughtæ ¼å¼ï¼‰
    Generate Chain-of-Thought reasoning data
    """
    cot_prompts = []
    
    for case in cases:
        title = case.get("title", "")
        content = case.get("content", "")
        
        if not content or len(content) < 100:
            continue
        
        # æ„å»ºCoTæ ¼å¼
        cot = {
            "instruction": "è¯·ä½¿ç”¨å…ƒæ°”è„‰æ³•çš„æ€ç»´æ–¹å¼ï¼Œé€æ­¥åˆ†æä»¥ä¸‹ç—…ä¾‹ï¼Œç»™å‡ºè¯Šæ–­å’Œæ²»ç–—æ–¹æ¡ˆã€‚",
            "input": f"ç—…ä¾‹ä¿¡æ¯ï¼š{title}",
            "output": f"""è®©æˆ‘æŒ‰ç…§å…ƒæ°”è„‰æ³•çš„æ€ç»´æ­¥éª¤æ¥åˆ†æè¿™ä¸ªç—…ä¾‹ï¼š

**ç¬¬ä¸€æ­¥ï¼šè„‰è±¡åˆ†æ**
é¦–å…ˆï¼Œæˆ‘éœ€è¦è¯„ä¼°æ‚£è€…çš„å…ƒæ°”çŠ¶æ€ã€‚æ ¹æ®å…ƒæ°”è„‰æ³•çš„"å¯Ÿæ ¹"åŸåˆ™ï¼Œé‡ç‚¹å…³æ³¨æ²‰å–å±‚æ¬¡çš„è„‰è±¡ã€‚

**ç¬¬äºŒæ­¥ï¼šå…ƒæ°”çŠ¶æ€åˆ¤æ–­**
æ ¹æ®è„‰è±¡ç‰¹å¾åˆ¤æ–­å…ƒæ°”æ˜¯å……ç››ã€è™šæŸè¿˜æ˜¯å¤–æµ®ã€‚

**ç¬¬ä¸‰æ­¥ï¼šè¾¨è¯è®ºæ²»**
{content}

**ç¬¬å››æ­¥ï¼šæ²»ç–—æ–¹æ¡ˆ**
æ ¹æ®å…ƒæ°”çŠ¶æ€ç¡®å®šæ²»åˆ™ï¼Œé€‰æ‹©ç›¸åº”çš„æ–¹å‰‚ã€‚

ä»¥ä¸Šæ˜¯å…ƒæ°”è„‰æ³•çš„è¯Šæ–­æ€è·¯å’Œæ²»ç–—å»ºè®®ã€‚"""
        }
        cot_prompts.append(cot)
    
    return cot_prompts


def generate_conversation_data(theories: List[Dict], cases: List[Dict]) -> List[Dict]:
    """
    ç”Ÿæˆå¯¹è¯æ ¼å¼æ•°æ®ï¼ˆé€‚ç”¨äºChatMLæ ¼å¼å¾®è°ƒï¼‰
    Generate conversation format data for ChatML fine-tuning
    """
    conversations = []
    
    # ç†è®ºé—®ç­”å¯¹è¯
    for theory in theories[:50]:  # é™åˆ¶æ•°é‡
        title = theory.get("title", "")
        content = theory.get("content", "")
        
        if not title or not content:
            continue
        
        conv = {
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç²¾é€šå…ƒæ°”è„‰æ³•çš„ä¸­åŒ»å¸ˆï¼Œè¯·æ ¹æ®å…ƒæ°”è„‰æ³•çš„ç†è®ºä½“ç³»å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": f"è¯·è§£é‡Š{title}"},
                {"role": "assistant", "content": content.strip()}
            ]
        }
        conversations.append(conv)
    
    # ç—…ä¾‹åˆ†æå¯¹è¯
    for case in cases[:30]:
        title = case.get("title", "")
        content = case.get("content", "")
        
        if not content:
            continue
        
        conv = {
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç²¾é€šå…ƒæ°”è„‰æ³•çš„ä¸­åŒ»å¸ˆã€‚è¯·ä½¿ç”¨å…ƒæ°”è„‰æ³•çš„è¯Šæ–­æ€è·¯åˆ†æç—…ä¾‹ï¼Œé‡ç‚¹å…³æ³¨è„‰è±¡çš„æµ®ä¸­æ²‰ä¸‰å±‚ç‰¹å¾å’Œå…ƒæ°”çŠ¶æ€è¯„ä¼°ã€‚"},
                {"role": "user", "content": f"è¯·åˆ†æè¿™ä¸ªç—…ä¾‹ï¼š{title}"},
                {"role": "assistant", "content": content.strip()}
            ]
        }
        conversations.append(conv)
    
    return conversations


def prepare_training_data(
    import_dir: str,
    output_dir: str
) -> Dict[str, Any]:
    """
    å‡†å¤‡å®Œæ•´çš„è®­ç»ƒæ•°æ®é›†
    Prepare complete training dataset
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½å·²å¯¼å…¥æ•°æ®...")
    data = load_imported_data(import_dir)
    
    print(f"   ç†è®ºæ¡ç›®: {len(data['theories'])}")
    print(f"   ç—…ä¾‹æ¡ç›®: {len(data['cases'])}")
    
    stats = {
        "source_theories": len(data['theories']),
        "source_cases": len(data['cases']),
        "generated_prompts": {}
    }
    
    # 1. ç”ŸæˆAlpacaæ ¼å¼æ•°æ®ï¼ˆç”¨äºé€šç”¨å¾®è°ƒï¼‰
    print("\nğŸ”„ ç”ŸæˆAlpacaæ ¼å¼è®­ç»ƒæ•°æ®...")
    theory_prompts = generate_theory_prompts(data['theories'])
    case_prompts = generate_case_prompts(data['cases'])
    all_prompts = theory_prompts + case_prompts
    
    alpaca_file = os.path.join(output_dir, "alpaca_yuanqi.json")
    with open(alpaca_file, 'w', encoding='utf-8') as f:
        json.dump(all_prompts, f, ensure_ascii=False, indent=2)
    
    print(f"   âœ… Alpacaæ ¼å¼: {len(all_prompts)} æ¡ -> {alpaca_file}")
    stats["generated_prompts"]["alpaca"] = len(all_prompts)
    
    # 2. ç”ŸæˆCoTæ ¼å¼æ•°æ®
    print("\nğŸ”„ ç”Ÿæˆæ€ç»´é“¾æ ¼å¼æ•°æ®...")
    cot_prompts = generate_cot_prompts(data['cases'])
    
    cot_file = os.path.join(output_dir, "cot_yuanqi.json")
    with open(cot_file, 'w', encoding='utf-8') as f:
        json.dump(cot_prompts, f, ensure_ascii=False, indent=2)
    
    print(f"   âœ… CoTæ ¼å¼: {len(cot_prompts)} æ¡ -> {cot_file}")
    stats["generated_prompts"]["cot"] = len(cot_prompts)
    
    # 3. ç”Ÿæˆå¯¹è¯æ ¼å¼æ•°æ®ï¼ˆChatMLï¼‰
    print("\nğŸ”„ ç”Ÿæˆå¯¹è¯æ ¼å¼æ•°æ®...")
    conversations = generate_conversation_data(data['theories'], data['cases'])
    
    conv_file = os.path.join(output_dir, "conversations_yuanqi.json")
    with open(conv_file, 'w', encoding='utf-8') as f:
        json.dump(conversations, f, ensure_ascii=False, indent=2)
    
    print(f"   âœ… å¯¹è¯æ ¼å¼: {len(conversations)} æ¡ -> {conv_file}")
    stats["generated_prompts"]["conversations"] = len(conversations)
    
    # 4. ç”ŸæˆJSONLæ ¼å¼ï¼ˆé€šç”¨æ ¼å¼ï¼‰
    print("\nğŸ”„ ç”ŸæˆJSONLæ ¼å¼...")
    jsonl_file = os.path.join(output_dir, "train_yuanqi.jsonl")
    with open(jsonl_file, 'w', encoding='utf-8') as f:
        for item in all_prompts:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"   âœ… JSONLæ ¼å¼: {len(all_prompts)} è¡Œ -> {jsonl_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats_file = os.path.join(output_dir, "training_stats.json")
    stats["generated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    return stats


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="å‡†å¤‡å…ƒæ°”è„‰æ³•AIè®­ç»ƒæ•°æ®")
    parser.add_argument(
        "--input", "-i",
        default="data/corpus/yuanqi_imported",
        help="å·²å¯¼å…¥æ•°æ®ç›®å½•"
    )
    parser.add_argument(
        "--output", "-o",
        default="data/training",
        help="è®­ç»ƒæ•°æ®è¾“å‡ºç›®å½•"
    )
    
    args = parser.parse_args()
    
    print("=" * 50)
    print("å…ƒæ°”è„‰æ³•AIè®­ç»ƒæ•°æ®å‡†å¤‡")
    print("=" * 50)
    
    stats = prepare_training_data(args.input, args.output)
    
    print("\n" + "=" * 50)
    print("ğŸ“Š ç”Ÿæˆå®Œæˆ!")
    print(f"   åŸå§‹ç†è®º: {stats['source_theories']} æ¡")
    print(f"   åŸå§‹ç—…ä¾‹: {stats['source_cases']} æ¡")
    print(f"   Alpacaæ ¼å¼: {stats['generated_prompts']['alpaca']} æ¡")
    print(f"   CoTæ ¼å¼: {stats['generated_prompts']['cot']} æ¡")
    print(f"   å¯¹è¯æ ¼å¼: {stats['generated_prompts']['conversations']} æ¡")
    print("=" * 50)


if __name__ == "__main__":
    main()
